{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tiny Language Model Quest","text":"<p>Info</p> <p>\ud83d\udea7 work in progress</p>"},{"location":"hawaiian-tests.report/","title":"\ud83c\udf3a\ud83c\udf55 Hawaiian test Results","text":"<p>Generated on: 9/28/2025, 2:20:25 PM</p> Model Score Quantization Format Temperature Top P Total Time hf.co/menlo/jan-nano-gguf:q4_k_m 5 q4_k_m 0.00 0.80 1m35.76s hf.co/liquidai/lfm2-1.2b-gguf:q4_k_m 5 q4_k_m 0.00 0.80 1m18.80s hf.co/liquidai/lfm2-2.6b-gguf:q4_k_m 4 q4_k_m 0.00 0.80 1m43.04s hf.co/liquidai/lfm2-350m-gguf:q4_k_m 4 q4_k_m 0.00 0.80 1m21.25s hf.co/liquidai/lfm2-700m-gguf:q4_k_m 5 q4_k_m 0.00 0.80 1m25.47s hf.co/menlo/lucy-gguf:q4_k_m 5 q4_k_m 0.00 0.80 2m30.90s hf.co/qwen/qwen2.5-0.5b-instruct-gguf:q4_k_m 5 q4_k_m 0.00 0.80 2m1.23s hf.co/qwen/qwen2.5-1.5b-instruct-gguf:q4_k_m 5 q4_k_m 0.00 0.80 1m9.24s hf.co/qwen/qwen2.5-3b-instruct-gguf:q4_k_m 5 q4_k_m 0.00 0.80 1m19.11s"},{"location":"hawaiian-tests.report/#statistics","title":"Statistics","text":"<ul> <li>Total number of tested models: 9</li> <li>Average score: 4.78</li> <li>Maximum score: 5</li> <li>Minimum score: 4</li> </ul>"},{"location":"models-tests/","title":"Models Tests","text":"<p>Note</p> <p>More tests to come soon...</p>"},{"location":"models-tests/#hawaiian-test","title":"\ud83c\udf3a\ud83c\udf55 Hawaiian test","text":"<p>The Hawaiian Test is a simple, practical technique invented by Philippe Charri\u00e8re to verify the proper functioning of a generative AI setup\u2014specifically, to ensure your selected model actually follows instructions and deliver expected responses.</p>"},{"location":"models-tests/#purpose","title":"Purpose","text":"<ul> <li>To avoid wasted effort when building AI tools where the model ignores your instructions.</li> <li>To quickly check if system messages and prompt engineering work as expected for your model.</li> </ul>"},{"location":"models-tests/#how-the-test-works","title":"How the test works","text":"<p>It involves five main checks: Does the model itself respond as expected?</p> <ol> <li>who are you?</li> <li>who invented Hawaiian pizza?</li> <li>what are the ingredients of a Hawaiian pizza?</li> <li>what are the regional variations of Hawaiian pizza?</li> <li>what is the best pizza?</li> </ol>"},{"location":"models-tests/#why-hawaiian","title":"Why \"Hawaiian\"?","text":"<p>The name comes from the test prompt: you instruct the model (via a detailed system message) to behave as a Hawaiian pizza expert. You then ask questions about Hawaiian pizza. If the model answers as a knowledgeable, enthusiastic Hawaiian pizza expert and follows your constraints (focus on Hawaiian, correct history, detailed ingredients, etc.), the setup passes the test.</p>"},{"location":"models-tests/#typical-hawaiian-test-steps","title":"Typical \"Hawaiian Test\" steps","text":"<ol> <li>Send a detailed system prompt: e.g., \u201cYou are Bob, a Hawaiian pizza expert. Provide enthusiastic, accurate info about history, ingredients, and regional varieties...\u201d</li> <li>Ask targeted questions: \u201cWho are you?\u201d \u201cWho invented Hawaiian pizza?\u201d \u201cWhat are the ingredients?\u201d \u201cWhat are the regional variations?\u201d \u201cWhat is the best pizza?\u201d</li> <li>Evaluate responses: The AI should reply in character, only use provided knowledge, defend pineapple on pizza, and keep focus on Hawaiian pizza even when asked about other pizzas.</li> <li>Success: If the responses match your expectations, you can trust this stack for further development. If not, you know where troubleshooting is needed (API, model, or framework).</li> </ol> <p>In short, the Hawaiian Test is a fast, domain-specific way to validate prompt control and model behavior for LLM projects\u2014using a whimsical but demanding scenario as the benchmark.[Ref1]</p> <p>Info</p> <p>The Hawaiian Test Applied to Pydantic AI and Docker Model Runner: Ref1</p>"},{"location":"tlms-list/","title":"Tiny Language Models List","text":"<p>This list catalogs open-source Tiny Language Models (TLMs), which are small-sized language models. I haven't tested them all yet, but I'm listing them here for reference.</p> <p>Info</p> <p>\ud83e\udd1a only GGUF models, Text generation, RAG &amp; Tools (function calling)</p> Link Description Tested \ud83d\udcdd Liquid Nanos Library of task-specific models LFM2-1.2B-Extract-GGUF Extract information from unstructured documents no LFM2-350M-Extract-GGUF Extract information from unstructured documents no LFM2-1.2B-RAG-GGUF Answering questions based on provided documents no LFM2-1.2B-Tool-GGUF Concise and precise tool calling no LFM2-350M-Math-GGUF Tiny reasoning model designed for math problems no \ud83d\udcdd LFM2 Hybrid models designed for on-device deployment LFM2-2.6B-GGUF Hybrid models developed designed for edge AI \ud83c\udf3a LFM2-1.2B-GGUF Hybrid models developed designed for edge AI \ud83c\udf3a LFM2-700M-GGUF Hybrid models developed designed for edge AI \ud83c\udf3a LFM2-350M-GGUF Hybrid models developed designed for edge AI \ud83c\udf3a \ud83d\udcdd Menlo Menlo Research Jan-nano-128k-gguf Deeper research through context understanding no \ud83d\udc99 Jan-nano-gguf Excellent function calling and tool integration \ud83c\udf3a Lucy-128k-ggu Focused Reasoning + tools no \ud83d\udc99 Lucy-gguf Focused Reasoning + tools \ud83c\udf3a \ud83d\udcdd Qwen Qwen language models Qwen2.5-0.5B-Instruct-GGUF Instruction-tuned language model \ud83c\udf3a Qwen2.5-1.5B-Instruct-GGUF Instruction-tuned language model \ud83c\udf3a Qwen2.5-3B-Instruct-GGUF Instruction-tuned language model \ud83c\udf3a <p>Note</p> <p>\ud83d\udc4b If you know of others, feel free to open an issue or submit a PR.</p>"},{"location":"what-is-new/","title":"What's new with TLM Quest","text":"<p>Info</p> <p>\ud83d\udea7 work in progress</p>"}]}